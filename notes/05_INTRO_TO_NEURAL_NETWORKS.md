Had a ton of great notes that I lost somehow!


weights: multiplied times the inputs

output function: $\sum_{1}^N w_ix_i \cdot bias$

bias: Added to the input weights.

Activation Threshold: value over which the neuron fires

Activation function: given the inputs, multiplies times the weights, applies the bias, then compares to the Activation Threshold.

Backpropogation: weights are adjusted based on the training input and the predicted outcome variation.


e.g.

2 inputs to a neuron

$x_1=0.6, x_2=1.0, w_1 = 0.5, w_2=.8, Threshold = 1.0$

Transfer function: $x_1 * w_1 + x_2 * w_2$

$ = .3 + .8 = 1.1$

Comparison: Output - Threshold

$ 1.1 - 1.0 = .1$

Neuron is activated.
